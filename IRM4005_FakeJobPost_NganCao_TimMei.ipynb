# Fake Job Postings Detection using PyTorch
# Ngan Cao 101272381
# Tim Mei 101268588

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
import pandas as pd
import re
from collections import Counter
from sklearn.model_selection import train_test_split
from sklearn.metrics import precision_score, recall_score, f1_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt
import numpy as np

df = pd.read_csv('fake_job_postings.csv')

# Combine text fields
df['text'] = (
    df['title'].fillna('') + ' ' +
    df['description'].fillna('') + ' ' +
    df['requirements'].fillna('')
)

df = df[df['text'].str.strip() != ''].reset_index(drop=True)

# 2. DATASET CLASS
# Adapted from Abbas Akkasi's sample code (2025)

class JobDataset(Dataset):
    
    def __init__(self, texts, labels, vocab=None, max_length=150):
        self.texts = texts
        self.labels = labels
        self.max_length = max_length
        
        if vocab is None:
            self.vocab = self.build_vocab(texts)
        else:
            self.vocab = vocab

    def tokenize(self, text):
        # Adapted from Abbas Akkasi's sample code (2025)
        text = text.lower()
        text = re.sub(r'[^a-zA-Z\s]', '', text)
        return text.split()
    
    def build_vocab(self, texts, min_freq=2):
        # Adapted from Abbas Akkasi's sample code (2025)
        counter = Counter()
        for text in texts:
            counter.update(self.tokenize(text))
        
        vocab = {'<PAD>': 0, '<UNK>': 1}
        for word, count in counter.most_common(5000):
            if count >= min_freq:
                vocab[word] = len(vocab)
        return vocab
    
    def text_to_indices(self, text):
        # Adapted from Abbas Akkasi's sample code (2025)
        tokens = self.tokenize(text)
        indices = [self.vocab.get(t, 1) for t in tokens[:self.max_length]]
        indices.extend([0] * (self.max_length - len(indices)))
        return indices
    
    def __len__(self):
        # Adapted from Abbas Akkasi's sample code (2025)
        return len(self.texts)
    
    def __getitem__(self, idx):
        # Adapted from Abbas Akkasi's sample code (2025)
        return (
            torch.tensor(self.text_to_indices(self.texts[idx])),
            torch.tensor(self.labels[idx], dtype=torch.float)
        )

# training and split
texts = df['text'].tolist()
labels = df['fraudulent'].tolist()

train_texts, test_texts, train_labels, test_labels = train_test_split(
    texts, labels, test_size=0.2, random_state=42, stratify=labels
)

train_dataset = JobDataset(train_texts, train_labels)
test_dataset = JobDataset(test_texts, test_labels, vocab=train_dataset.vocab)

train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

# Model Definition
# Adapted from Abbas Akkasi's sample code (2025)

class FakeJobDetector(nn.Module):
    
    def __init__(self, vocab_size, embedding_dim=100, hidden_dim=128):
        # Adapted from Abbas Akkasi's sample code (2025)
        super(FakeJobDetector, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)
        self.lstm = nn.LSTM(
            embedding_dim, hidden_dim, batch_first=True, bidirectional=True
        )
        self.dropout = nn.Dropout(0.3)
        self.fc = nn.Linear(hidden_dim * 2, 1)
        self.sigmoid = nn.Sigmoid()
    
    def forward(self, x):
        # Adapted from Abbas Akkasi's sample code (2025)
        embedded = self.embedding(x)
        _, (hidden, _) = self.lstm(embedded)
        hidden = torch.cat((hidden[-2], hidden[-1]), dim=1)
        output = self.fc(self.dropout(hidden))
        return self.sigmoid(output).squeeze()

# Training Function
# Adapted from Abbas Akkasi's sample code (2025)

def train_model(model, train_loader, test_loader, epochs=10, lr=0.001):
    """Train the model and evaluate on test set"""
    
    device = torch.device('cpu')
    model = model.to(device)
    
    criterion = nn.BCELoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    
    # Lists to store metrics for plotting
    train_losses = []
    train_accuracies = []
    test_accuracies = []
    
    for epoch in range(epochs):
        # Training phase - Adapted from Abbas Akkasi's sample code (2025)
        model.train()
        total_loss = 0
        train_correct = 0
        train_total = 0
        
        for inputs, labels in train_loader:
            inputs, labels = inputs, labels
            
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
            predicted = (outputs > 0.5).float()
            train_total += labels.size(0)
            train_correct += (predicted == labels).sum().item()
        
        train_accuracy = 100 * train_correct / train_total
        avg_loss = total_loss / len(train_loader)
        
        # Evaluation phase - Adapted from Abbas Akkasi's sample code (2025)
        model.eval()
        test_correct = 0
        test_total = 0
        
        with torch.no_grad():
            for inputs, labels in test_loader:
                inputs, labels = inputs, labels
                outputs = model(inputs)
                predicted = (outputs > 0.5).float()
                test_total += labels.size(0)
                test_correct += (predicted == labels).sum().item()
        
        test_accuracy = 100 * test_correct / test_total
        
        # Store metrics
        train_losses.append(avg_loss)
        train_accuracies.append(train_accuracy)
        test_accuracies.append(test_accuracy)
        
        print(f"Epoch [{epoch+1:2d}/{epochs}] | "
              f"Loss: {avg_loss:.4f} | "
              f"Train Acc: {train_accuracy:.2f}% | "
              f"Test Acc: {test_accuracy:.2f}%")
    
    return model, train_losses, train_accuracies, test_accuracies

# Evaluation function

def evaluate_model(model, test_loader):
    """Evaluate model with precision, recall, and F1-score"""
    device = torch.device('cpu')
    model = model.to(device)
    model.eval()
    
    predictions = []
    true_labels = []
    
    with torch.no_grad():
        for inputs, labels in test_loader:
            inputs = inputs
            outputs = model(inputs)
            preds = (outputs > 0.5).long()
            
            predictions.extend(preds.numpy())
            true_labels.extend(labels.numpy())
    
    precision = precision_score(true_labels, predictions)
    recall = recall_score(true_labels, predictions)
    f1 = f1_score(true_labels, predictions)
    
    print("Evaluation Metrics:")
    print(f"Precision: {precision:.4f} ({precision*100:.2f}%)")
    print(f"Recall:    {recall:.4f} ({recall*100:.2f}%)")
    print(f"F1-Score:  {f1:.4f} ({f1*100:.2f}%)")
    
    print("\nClassification Report:")
    print(classification_report(true_labels, predictions, 
                                target_names=['Real', 'Fake'], digits=4))
    
    cm = confusion_matrix(true_labels, predictions)
    print("\nConfusion Matrix:")
    print(f"              Predicted")
    print(f"           Real    Fake")
    print(f"Real      {cm[0][0]:5d}  {cm[0][1]:5d}")
    print(f"Fake      {cm[1][0]:5d}  {cm[1][1]:5d}")
    
    return precision, recall, f1, cm

# Plotting functions

def plot_training_results(train_losses, train_accuracies, test_accuracies):
    """Visualize training progress"""
    
    epochs = range(1, len(train_losses) + 1)
    
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))
    
    # Plot 1: Training Loss
    ax1.plot(epochs, train_losses, 'b-', linewidth=2, label='Training Loss')
    ax1.set_xlabel('Epoch', fontsize=12)
    ax1.set_ylabel('Loss', fontsize=12)
    ax1.set_title('Training Loss Over Epochs', fontsize=14, fontweight='bold')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # Plot 2: Accuracy Comparison
    ax2.plot(epochs, train_accuracies, 'g-', linewidth=2, label='Training Accuracy')
    ax2.plot(epochs, test_accuracies, 'r-', linewidth=2, label='Test Accuracy')
    ax2.set_xlabel('Epoch', fontsize=12)
    ax2.set_ylabel('Accuracy (%)', fontsize=12)
    ax2.set_title('Training vs Test Accuracy', fontsize=14, fontweight='bold')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('training_results.png', dpi=300, bbox_inches='tight')
    print("\nTraining plots saved as 'training_results.png'")
    plt.show()

def plot_confusion_matrix(cm):
    """Visualize confusion matrix"""
    
    fig, ax = plt.subplots(figsize=(8, 6))
    
    im = ax.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)
    ax.figure.colorbar(im, ax=ax)
    
    ax.set(xticks=np.arange(cm.shape[1]),
           yticks=np.arange(cm.shape[0]),
           xticklabels=['Real', 'Fake'],
           yticklabels=['Real', 'Fake'],
           title='Confusion Matrix',
           ylabel='True Label',
           xlabel='Predicted Label')
    
    plt.setp(ax.get_xticklabels(), rotation=45, ha="right", rotation_mode="anchor")
    
    # Add text annotations
    thresh = cm.max() / 2.
    for i in range(cm.shape[0]):
        for j in range(cm.shape[1]):
            ax.text(j, i, format(cm[i, j], 'd'),
                   ha="center", va="center",
                   color="white" if cm[i, j] > thresh else "black",
                   fontsize=20, fontweight='bold')
    
    plt.tight_layout()
    plt.savefig('confusion_matrix.png', dpi=300, bbox_inches='tight')
    print("Confusion matrix saved as 'confusion_matrix.png'")
    plt.show()

def plot_metrics_comparison(precision, recall, f1):
    """Visualize evaluation metrics"""
    
    metrics = ['Precision', 'Recall', 'F1-Score']
    values = [precision, recall, f1]
    
    fig, ax = plt.subplots(figsize=(10, 6))
    
    bars = ax.bar(metrics, values, color=['#3498db', '#e74c3c', '#2ecc71'], 
                  width=0.6, edgecolor='black', linewidth=1.5)
    
    ax.set_ylabel('Score', fontsize=12)
    ax.set_title('Model Performance Metrics', fontsize=14, fontweight='bold')
    ax.set_ylim(0, 1.1)
    ax.grid(axis='y', alpha=0.3)
    
    # Add value labels on bars
    for bar, value in zip(bars, values):
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height,
                f'{value:.4f}\n({value*100:.2f}%)',
                ha='center', va='bottom', fontsize=11, fontweight='bold')
    
    plt.tight_layout()
    plt.savefig('metrics_comparison.png', dpi=300, bbox_inches='tight')
    print("Metrics comparison saved as 'metrics_comparison.png'")
    plt.show()

# model training

vocab_size = len(train_dataset.vocab)
model = FakeJobDetector(vocab_size)

model, train_losses, train_accs, test_accs = train_model(
    model, train_loader, test_loader, epochs=10, lr=0.001
)

# Plot training results
plot_training_results(train_losses, train_accs, test_accs)

# model evaluation
precision, recall, f1, cm = evaluate_model(model, test_loader)

# Plot confusion matrix and metrics
plot_confusion_matrix(cm)
plot_metrics_comparison(precision, recall, f1)

class JobAnalyzer:
    # Analyze job postings for fake/real classification    
    def __init__(self, trained_model, vocabulary, dataset):
        self.model = trained_model
        self.vocab = vocabulary
        self.dataset = dataset
        self.device = torch.device('cpu')
        self.model.to(self.device)
        self.model.eval()
    
    def analyze(self, job_text):
        indices = self.dataset.text_to_indices(job_text)
        tensor = torch.tensor([indices])
        
        with torch.no_grad():
            prob = self.model(tensor).item()
        
        is_fake = prob > 0.5
        confidence = prob if is_fake else 1 - prob
        
        if prob >= 0.8:
            risk = "High risk"
        elif prob >= 0.6:
            risk = "Medium risk"
        elif prob >= 0.4:
            risk = "Low risk"
        else:
            risk = "Real job posting"
        
        return {
            'prediction': 'FAKE' if is_fake else 'REAL',
            'confidence': confidence,
            'fake_prob': prob,
            'real_prob': 1 - prob,
            'risk_level': risk
        }
    
    def display_analysis(self, job_text):
        result = self.analyze(job_text)
        
        print("Job analysis result:")
        print(f"Text: {job_text[:80]}...")
        print(f"\nRisk Level: {result['risk_level']}")
        print(f"Prediction: {result['prediction']}")
        print(f"Confidence: {result['confidence']*100:.1f}%")
        print(f"Fake probability: {result['fake_prob']*100:.1f}%")
        print(f"Real probability: {result['real_prob']*100:.1f}%")
 
analyzer = JobAnalyzer(model, train_dataset.vocab, train_dataset)

test_jobs = [
    "Software Engineer at Google. 5+ years experience. Salary $150k-200k. Apply on our website.",
    "WORK FROM HOME!!! Make $10000 weekly!!! No experience needed!!! Send payment for training!!!",
    "Data Analyst position. Bachelor's degree required. Salary $70k. Benefits included.",
    "Easy money! Just send your bank details! Start earning today! No questions asked!",
]

for i, job in enumerate(test_jobs, 1):
    print(f"\nExample {i}:")
    analyzer.display_analysis(job)

def save_trained_model(model, vocab, filename='job_detector.pth'):
    checkpoint = {
        'state_dict': model.state_dict(),
        'vocabulary': vocab,
        'config': {
            'vocab_size': len(vocab),
            'embedding_dim': 100,
            'hidden_dim': 128
        }
    }
    torch.save(checkpoint, filename)

save_trained_model(model, train_dataset.vocab)


print("\nMixed Example Area:\n")
# Select 3 completely random posts from the dataset (mixed real + fake)
mixed_samples = df.sample(3, random_state=42)

for i, row in mixed_samples.iterrows():
    print(f"Example {i}:\n")
    print("Title:", row['title'])
    print("Description:", row['description'])
    print("Full Text:", row['text'], "\n")
    
    print("Model Analysis:")
    analyzer.display_analysis(row['text'])


print("\n Summary of the results \n")
print(f"Dataset: {len(df)} job postings\n")
print(f"Training samples: {len(train_dataset)}\n")
print(f"Test samples: {len(test_dataset)}\n")
print(f"Vocabulary size: {vocab_size}\n")
print(f"Epochs trained: 10\n")
print(f"Final Precision: {precision:.4f}\n")
print(f"Final Recall: {recall:.4f}\n")
print(f"Final F1-Score: {f1:.4f}\n")
